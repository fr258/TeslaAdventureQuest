[2021-02-20 22:00:04,719] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:04,729] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:00:04,729] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 6: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:14,719] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:14,724] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:00:14,725] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 7: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:24,720] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:24,724] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:00:24,724] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 8: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:34,720] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:34,724] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:00:34,724] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 9: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:44,721] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:44,724] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:00:44,725] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 10: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:54,721] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:00:54,725] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:00:54,725] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 11: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:04,720] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:04,724] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:01:04,725] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 12: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:14,721] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:14,725] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:01:14,725] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 13: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:24,722] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:24,726] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:01:24,727] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 14: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:34,722] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:34,726] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:01:34,727] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 15: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:44,722] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:44,727] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:01:44,727] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 16: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:54,724] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:01:54,727] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:01:54,728] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 17: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:04,724] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:04,727] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:02:04,728] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 18: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:14,724] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:14,728] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:02:14,729] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 19: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:24,725] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:24,733] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:02:24,733] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 20: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:34,726] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:34,729] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:02:34,730] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 21: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:44,726] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:44,730] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:02:44,731] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 22: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:54,728] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:02:54,732] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:02:54,733] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 23: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:04,728] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:04,732] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:03:04,732] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 24: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:14,728] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:14,732] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:03:14,732] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 25: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:24,729] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:24,733] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:03:24,733] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 26: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:34,729] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:34,732] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:03:34,733] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 27: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:44,729] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:44,732] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 0 for partition test-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:03:44,733] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 28: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:46,548] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2021-02-20 22:03:46,551] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2021-02-20 22:03:46,568] INFO Stopped http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2021-02-20 22:03:46,569] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2021-02-20 22:03:46,575] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2021-02-20 22:03:46,576] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:102)
[2021-02-20 22:03:46,577] INFO Stopping task test6-0 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,580] INFO Stopping task test6-1 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,581] INFO Stopping task test6-2 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,581] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,583] WARN WorkerSinkTask{id=test6-0} Offset commit failed during close (org.apache.kafka.connect.runtime.WorkerSinkTask:395)
[2021-02-20 22:03:46,585] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,583] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,583] INFO Stopping task test6-3 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,587] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Member connector-consumer-test6-1-a3dbe9c2-6017-4b39-bc2a-d41f898b09e5 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,586] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,586] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 29: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closePartitions(WorkerSinkTask.java:644)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:202)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'test': was expecting 'null', 'true', 'false' or NaN
 at [Source: (String)"test"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2853)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2831)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken2(ReaderBasedJsonParser.java:2628)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchToken(ReaderBasedJsonParser.java:2606)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._matchTrue(ReaderBasedJsonParser.java:2564)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:725)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:03:46,591] INFO Stopping task test6-4 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,589] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Member connector-consumer-test6-2-114757c7-c142-4224-8d7a-1561aa154bba sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,589] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,595] INFO Stopping task test6-5 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,593] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,595] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,595] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,598] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,600] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,598] INFO Stopping task test6-6 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,596] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,603] INFO Stopping task test6-7 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,602] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Member connector-consumer-test6-5-49d6c353-a91e-4675-9bcb-1c28f0709b36 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,605] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,601] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Member connector-consumer-test6-3-97b64edb-c1e4-4576-bc95-b3b5c7295641 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,599] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,605] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,607] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Member connector-consumer-test6-7-d5bfbe3b-1ee5-4310-bd2b-ca9c2745b4a3 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,605] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,608] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,610] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,611] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,612] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,612] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,605] INFO Stopping task test6-8 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,604] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,604] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Revoke previously assigned partitions test-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:03:46,603] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,620] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,622] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,620] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,623] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,618] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,617] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,616] INFO Stopping task test6-9 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:03:46,607] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Member connector-consumer-test6-4-d6be4246-a3e5-41d8-a860-24ef766d17bc sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,627] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,626] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,625] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,628] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Member connector-consumer-test6-8-c79e31f4-e290-41bb-b467-e6e6220afeb0 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,621] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,621] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Member connector-consumer-test6-0-63f8c4f9-fd71-4b15-a105-8a11b1f9e03b sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,641] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Member connector-consumer-test6-6-f4d3c3f7-b1b2-46a3-a1df-24c201a07c8c sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,638] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:03:46,637] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,646] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:03:46,650] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Member connector-consumer-test6-9-151ed322-9c7f-4a7b-83b3-b29a405cd193 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:03:46,656] INFO App info kafka.consumer for connector-consumer-test6-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,663] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,663] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,666] INFO App info kafka.consumer for connector-consumer-test6-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,667] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,667] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,669] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,671] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,671] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,673] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,674] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,675] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,676] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,677] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,678] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,678] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,679] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,700] INFO App info kafka.consumer for connector-consumer-test6-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,730] INFO App info kafka.consumer for connector-consumer-test6-9 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,738] INFO App info kafka.consumer for connector-consumer-test6-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,742] INFO App info kafka.consumer for connector-consumer-test6-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,746] INFO App info kafka.consumer for connector-consumer-test6-8 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,750] INFO App info kafka.consumer for connector-consumer-test6-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,752] INFO App info kafka.consumer for connector-consumer-test6-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,754] INFO App info kafka.consumer for connector-consumer-test6-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,817] INFO Stopping connector test6 (org.apache.kafka.connect.runtime.Worker:387)
[2021-02-20 22:03:46,818] INFO Scheduled shutdown for WorkerConnector{id=test6} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2021-02-20 22:03:46,818] INFO Completed shutdown for WorkerConnector{id=test6} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2021-02-20 22:03:46,820] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:209)
[2021-02-20 22:03:46,822] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2021-02-20 22:03:46,822] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:03:46,823] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:03:46,824] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:03:46,825] INFO App info kafka.connect for 127.0.1.1:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:03:46,825] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:230)
[2021-02-20 22:03:46,830] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:120)
[2021-02-20 22:03:46,830] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2021-02-20 22:05:45,199] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2021-02-20 22:05:45,236] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../logs, -Dlog4j.configuration=file:./kafka_2.13-2.7.0/bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_281, 25.281-b09
	jvm.classpath = /home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/activation-1.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/argparse4j-0.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/audience-annotations-0.5.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-cli-1.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-lang3-3.8.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-api-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-basic-auth-extension-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-file-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-json-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-client-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-runtime-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-transforms-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-api-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-locator-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-utils-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-core-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-databind-2.10.5.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-paranamer-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-scala_2.13-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.inject-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.25.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.26.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.servlet-api-3.1.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jaxb-api-2.3.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-client-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-common-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-core-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-hk2-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-media-jaxb-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-server-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-client-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-continuation-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-http-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-io-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-security-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-server-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlet-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlets-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-util-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jopt-simple-5.0.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0-sources.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-clients-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-log4j-appender-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-raft-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-examples-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-scala_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-test-utils-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-tools-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/log4j-1.2.17.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/lz4-java-1.7.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/maven-artifact-3.6.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/metrics-core-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-buffer-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-codec-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-handler-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-resolver-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-epoll-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-unix-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/paranamer-2.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/plexus-utils-3.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/reflections-0.9.12.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/rocksdbjni-5.18.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-collection-compat_2.13-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-java8-compat_2.13-0.9.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-library-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-logging_2.13-3.9.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-reflect-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-api-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/snappy-java-1.1.7.7.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-jute-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zstd-jni-1.4.5-6.jar
	os.spec = Linux, amd64, 4.4.0-103-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-02-20 22:05:45,242] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2021-02-20 22:05:45,507] INFO Loading plugin from: /home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-02-20 22:05:51,895] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:05:51,898] INFO Added plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:51,900] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:51,900] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:51,901] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:51,902] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:51,902] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:56,990] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:05:56,991] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:56,992] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:56,992] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:56,993] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:56,998] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:56,999] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:56,999] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,000] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,001] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,001] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,002] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,003] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,003] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,004] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,005] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,006] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,006] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,007] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,008] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,009] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,010] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,011] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,011] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,012] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,013] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,013] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,014] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,015] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,016] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,017] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,017] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,018] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,019] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,020] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,020] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,021] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,022] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,023] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,023] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,024] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,025] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,026] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,026] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,027] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,028] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,029] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,030] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:05:57,034] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,035] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,037] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,038] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,039] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,040] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,041] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,042] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,042] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,043] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,045] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,045] INFO Added aliases 'RocksetSinkConnector' and 'RocksetSink' to plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,046] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,047] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,049] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,050] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,051] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,052] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,053] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,054] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,055] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,056] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,057] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,058] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,059] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,059] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,060] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,061] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,062] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,063] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,066] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,068] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,071] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,073] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,075] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,076] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,076] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,077] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,078] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:05:57,079] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,080] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,080] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:05:57,225] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [kafka-connect-rockset-1.2.0-jar-with-dependencies.jar]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:361)
[2021-02-20 22:05:57,229] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:05:57,249] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:05:57,704] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:05:57,705] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:05:57,705] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:05:57,706] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:05:57,706] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:05:57,707] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:05:57,708] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:05:57,711] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:05:57,711] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:05:57,712] INFO Kafka startTimeMs: 1613876757708 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:05:59,479] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:05:59,495] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:05:59,552] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:05:59,554] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:05:59,555] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:05:59,618] INFO Logging initialized @16248ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2021-02-20 22:05:59,930] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-02-20 22:05:59,931] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-02-20 22:05:59,975] INFO jetty-9.4.33.v20201020; built: 2020-10-20T23:39:24.803Z; git: 1be68755656cef678b79a2ef1c2ebbca99e25420; jvm 1.8.0_281-b09 (org.eclipse.jetty.server.Server:375)
[2021-02-20 22:06:00,132] INFO Started http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-02-20 22:06:00,135] INFO Started @16763ms (org.eclipse.jetty.server.Server:415)
[2021-02-20 22:06:00,275] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:06:00,276] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-02-20 22:06:00,277] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:06:00,278] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-02-20 22:06:00,279] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:06:00,281] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-02-20 22:06:00,323] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:06:00,325] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:06:00,352] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:06:00,353] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:06:00,353] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:06:00,354] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:06:00,354] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:06:00,355] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:06:00,355] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:06:00,357] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:00,358] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:00,360] INFO Kafka startTimeMs: 1613876760356 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:00,606] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:06:00,609] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:06:00,638] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:06:00,640] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:06:00,641] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:06:00,661] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:00,661] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:00,662] INFO Kafka startTimeMs: 1613876760660 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:01,293] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:06:01,299] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:06:01,339] INFO Kafka Connect standalone worker initialization took 16130ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2021-02-20 22:06:01,340] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-02-20 22:06:01,345] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-02-20 22:06:01,346] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2021-02-20 22:06:01,349] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-02-20 22:06:01,360] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2021-02-20 22:06:01,361] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-02-20 22:06:01,361] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-02-20 22:06:01,565] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-02-20 22:06:01,980] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-02-20 22:06:01,981] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-02-20 22:06:01,986] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2021-02-20 22:06:04,455] INFO Started o.e.j.s.ServletContextHandler@2b0b4d53{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2021-02-20 22:06:04,455] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-02-20 22:06:04,456] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-02-20 22:06:04,536] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2021-02-20 22:06:04,589] INFO Creating connector test6 of type rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2021-02-20 22:06:04,593] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:04,596] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:04,622] INFO Instantiated connector test6 with version 1.0 of type class rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2021-02-20 22:06:04,624] INFO Finished creating connector test6 (org.apache.kafka.connect.runtime.Worker:310)
[2021-02-20 22:06:04,630] INFO Starting RocksetSinkConnector (rockset.RocksetSinkConnector:22)
[2021-02-20 22:06:04,634] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:04,635] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:04,643] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:04,645] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:04,656] INFO Creating task test6-0 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:04,666] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:04,667] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:04,672] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:04,675] INFO Instantiated task test6-0 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:04,680] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:04,681] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:04,682] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-0 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:04,683] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-0 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:04,685] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:04,707] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:04,709] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:04,710] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:04,762] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:04,960] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:04,961] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:04,962] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:04,962] INFO Kafka startTimeMs: 1613876764961 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,006] INFO Creating task test6-1 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,011] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,012] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,013] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,014] INFO Instantiated task test6-1 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,015] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,016] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,017] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-1 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,017] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-1 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,019] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-1 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,020] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,026] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,025] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,028] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,028] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,030] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,035] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,073] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,074] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,075] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,076] INFO Kafka startTimeMs: 1613876765074 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,091] INFO Creating task test6-2 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,093] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,098] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,099] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,101] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,110] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,113] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,114] INFO Instantiated task test6-2 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,115] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,116] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,117] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-2 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,118] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-2 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,119] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-2 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,128] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,131] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,133] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,136] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,182] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,183] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,183] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,184] INFO Kafka startTimeMs: 1613876765182 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,197] INFO Creating task test6-3 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,202] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,204] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,210] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,211] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,211] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,220] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,225] INFO Instantiated task test6-3 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,226] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,227] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,227] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-3 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,228] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-3 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,228] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-3 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,238] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,247] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,249] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,255] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,280] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,282] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,283] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,284] INFO Kafka startTimeMs: 1613876765282 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,297] INFO Creating task test6-4 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,304] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,310] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,311] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,313] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,320] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,326] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,327] INFO Instantiated task test6-4 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,328] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,330] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,331] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-4 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,332] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-4 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,333] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-4 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,340] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,342] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,345] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,349] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,382] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,384] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,384] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,385] INFO Kafka startTimeMs: 1613876765383 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,399] INFO Creating task test6-5 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,402] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,412] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,414] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,414] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,415] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,425] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,428] INFO Instantiated task test6-5 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,429] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,430] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,431] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-5 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,431] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-5 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,432] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-5 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,436] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,438] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,439] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,441] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,470] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,471] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,471] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,471] INFO Kafka startTimeMs: 1613876765470 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,482] INFO Creating task test6-6 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,484] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,488] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,490] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,488] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,494] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,496] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,497] INFO Instantiated task test6-6 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,498] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,499] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,499] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-6 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,500] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-6 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,501] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-6 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,505] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,506] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,508] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,511] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,540] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,542] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,543] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,543] INFO Kafka startTimeMs: 1613876765542 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,552] INFO Creating task test6-7 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,559] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,565] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,566] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,566] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,570] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,568] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,573] INFO Instantiated task test6-7 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,575] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,580] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,581] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-7 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,582] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-7 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,583] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-7 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,588] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,590] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,593] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,595] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,623] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,624] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,625] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,625] INFO Kafka startTimeMs: 1613876765624 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,636] INFO Creating task test6-8 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,641] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,646] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,647] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,648] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,652] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,655] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,656] INFO Instantiated task test6-8 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,657] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,658] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,658] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-8 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,659] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-8 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,660] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-8 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,665] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,668] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,678] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,682] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,705] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,706] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,706] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,707] INFO Kafka startTimeMs: 1613876765706 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,718] INFO Creating task test6-9 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:06:05,721] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,725] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,725] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:06:05,726] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,729] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,732] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:06:05,733] INFO Instantiated task test6-9 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:06:05,734] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,735] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:06:05,736] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-9 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:06:05,736] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-9 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:06:05,737] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-9 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:06:05,742] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:06:05,754] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:06:05,758] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:06:05,763] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:06:05,786] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:06:05,787] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:06:05,788] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:06:05,788] INFO Kafka startTimeMs: 1613876765787 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:06:05,807] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:06:05,810] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:06:05,811] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:06:05,812] INFO Created connector test6 (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2021-02-20 22:06:07,153] INFO WorkerSinkTask{id=test6-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,155] INFO WorkerSinkTask{id=test6-3} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,153] INFO WorkerSinkTask{id=test6-8} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,156] INFO WorkerSinkTask{id=test6-4} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,153] INFO WorkerSinkTask{id=test6-6} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,155] INFO WorkerSinkTask{id=test6-7} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,155] INFO WorkerSinkTask{id=test6-2} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,153] INFO WorkerSinkTask{id=test6-1} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,153] INFO WorkerSinkTask{id=test6-9} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,155] INFO WorkerSinkTask{id=test6-5} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:06:07,296] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,299] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,297] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,299] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,299] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,298] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,315] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,316] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,298] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,321] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,296] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,296] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,297] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:06:07,334] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,336] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,333] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,314] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,311] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,311] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,311] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,348] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,344] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,344] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,339] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:06:07,355] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,356] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,359] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,361] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,364] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,370] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,459] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,459] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,463] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,468] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,470] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,474] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,480] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-test6-6-51523442-9ebc-4de4-9e70-501a7153241f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,484] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,485] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,492] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,498] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,501] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Finished assignment for group at generation 1: {connector-consumer-test6-6-51523442-9ebc-4de4-9e70-501a7153241f=Assignment(partitions=[test2-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:06:07,531] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='connector-consumer-test6-6-51523442-9ebc-4de4-9e70-501a7153241f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:781)
[2021-02-20 22:06:07,534] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:472)
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
[2021-02-20 22:06:07,541] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:06:07,550] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-2-27df81f8-e94c-485c-8296-6a9714f6d958', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,550] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-3-95927d84-6e94-4477-a0ad-90362dfeaa14', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,550] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-5-1107c209-532d-4d27-8562-842f9b8ecd88', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,553] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-1-27a6c478-59be-454a-8097-2c4c3b00b7f7', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,553] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-4-f078f94d-ff70-4aec-b7b3-1b53ab85d1bd', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,553] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-0-a3c19cc4-ace5-4b79-8637-5747a7e2aab4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,552] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-9-dc940660-583b-40c9-856d-2bd8b37879e7', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,551] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-6-51523442-9ebc-4de4-9e70-501a7153241f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,640] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-7-041734d6-e6c5-4824-8b48-2b70f1b8775d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,558] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test6-8-ebe1a86d-e686-4813-99e7-0849ed939a6e', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:06:07,658] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Finished assignment for group at generation 2: {connector-consumer-test6-5-1107c209-532d-4d27-8562-842f9b8ecd88=Assignment(partitions=[]), connector-consumer-test6-4-f078f94d-ff70-4aec-b7b3-1b53ab85d1bd=Assignment(partitions=[]), connector-consumer-test6-9-dc940660-583b-40c9-856d-2bd8b37879e7=Assignment(partitions=[]), connector-consumer-test6-3-95927d84-6e94-4477-a0ad-90362dfeaa14=Assignment(partitions=[]), connector-consumer-test6-6-51523442-9ebc-4de4-9e70-501a7153241f=Assignment(partitions=[]), connector-consumer-test6-0-a3c19cc4-ace5-4b79-8637-5747a7e2aab4=Assignment(partitions=[test2-0]), connector-consumer-test6-2-27df81f8-e94c-485c-8296-6a9714f6d958=Assignment(partitions=[]), connector-consumer-test6-7-041734d6-e6c5-4824-8b48-2b70f1b8775d=Assignment(partitions=[]), connector-consumer-test6-8-ebe1a86d-e686-4813-99e7-0849ed939a6e=Assignment(partitions=[]), connector-consumer-test6-1-27a6c478-59be-454a-8097-2c4c3b00b7f7=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:06:07,670] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-9-dc940660-583b-40c9-856d-2bd8b37879e7', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,670] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-2-27df81f8-e94c-485c-8296-6a9714f6d958', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,674] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-1-27a6c478-59be-454a-8097-2c4c3b00b7f7', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,673] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-4-f078f94d-ff70-4aec-b7b3-1b53ab85d1bd', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,680] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,679] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-0-a3c19cc4-ace5-4b79-8637-5747a7e2aab4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,679] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-6-51523442-9ebc-4de4-9e70-501a7153241f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,678] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-5-1107c209-532d-4d27-8562-842f9b8ecd88', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,683] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[test2-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,684] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,686] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,689] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,677] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-3-95927d84-6e94-4477-a0ad-90362dfeaa14', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,675] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,675] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-7-041734d6-e6c5-4824-8b48-2b70f1b8775d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,693] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,683] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,694] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,682] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,698] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,681] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test6-8-ebe1a86d-e686-4813-99e7-0849ed939a6e', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:06:07,681] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,693] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,693] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,701] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:06:07,700] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,703] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,702] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,702] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,723] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Adding newly assigned partitions: test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:06:07,744] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Found no committed offset for partition test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2021-02-20 22:06:07,794] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Resetting offset for partition test2-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2021-02-20 22:11:05,366] INFO WorkerSinkTask{id=test6-0} Committing offsets asynchronously using sequence number 30: {test2-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:11:45,385] INFO WorkerSinkTask{id=test6-0} Committing offsets asynchronously using sequence number 34: {test2-0=OffsetAndMetadata{offset=2, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:12:15,390] INFO WorkerSinkTask{id=test6-0} Committing offsets asynchronously using sequence number 37: {test2-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:14:31,264] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2021-02-20 22:14:31,265] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2021-02-20 22:14:31,285] INFO Stopped http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2021-02-20 22:14:31,286] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2021-02-20 22:14:31,292] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2021-02-20 22:14:31,293] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:102)
[2021-02-20 22:14:31,295] INFO Stopping task test6-0 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,296] INFO Stopping task test6-1 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,297] INFO Stopping task test6-2 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,298] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,301] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,302] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,299] INFO Stopping task test6-3 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,299] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,306] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,306] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Member connector-consumer-test6-1-27a6c478-59be-454a-8097-2c4c3b00b7f7 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,302] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,308] INFO Stopping task test6-4 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,307] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,310] INFO Stopping task test6-5 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,306] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,311] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Member connector-consumer-test6-3-95927d84-6e94-4477-a0ad-90362dfeaa14 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,310] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,310] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Revoke previously assigned partitions test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:14:31,314] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,316] INFO Stopping task test6-6 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,312] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Member connector-consumer-test6-2-27df81f8-e94c-485c-8296-6a9714f6d958 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,317] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Member connector-consumer-test6-4-f078f94d-ff70-4aec-b7b3-1b53ab85d1bd sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,316] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,315] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Member connector-consumer-test6-0-a3c19cc4-ace5-4b79-8637-5747a7e2aab4 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,320] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,319] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,321] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,322] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,323] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,324] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,319] INFO Stopping task test6-7 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,327] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,328] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,322] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Member connector-consumer-test6-6-51523442-9ebc-4de4-9e70-501a7153241f sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,322] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Member connector-consumer-test6-5-1107c209-532d-4d27-8562-842f9b8ecd88 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,332] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,333] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,334] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Member connector-consumer-test6-7-041734d6-e6c5-4824-8b48-2b70f1b8775d sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,321] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,321] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,336] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,336] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,332] INFO Stopping task test6-8 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,329] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,340] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,341] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,342] INFO Stopping task test6-9 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:14:31,342] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Member connector-consumer-test6-8-ebe1a86d-e686-4813-99e7-0849ed939a6e sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,343] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:14:31,344] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,344] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,344] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:14:31,346] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,347] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,347] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Member connector-consumer-test6-9-dc940660-583b-40c9-856d-2bd8b37879e7 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:14:31,347] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,349] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,347] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,353] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,357] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,358] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,358] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,358] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,359] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,361] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,359] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,362] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,362] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,363] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,364] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,366] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,395] INFO App info kafka.consumer for connector-consumer-test6-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,446] INFO App info kafka.consumer for connector-consumer-test6-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,457] INFO App info kafka.consumer for connector-consumer-test6-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,469] INFO App info kafka.consumer for connector-consumer-test6-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,471] INFO App info kafka.consumer for connector-consumer-test6-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,474] INFO App info kafka.consumer for connector-consumer-test6-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,476] INFO App info kafka.consumer for connector-consumer-test6-9 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,478] INFO App info kafka.consumer for connector-consumer-test6-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,480] INFO App info kafka.consumer for connector-consumer-test6-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,482] INFO App info kafka.consumer for connector-consumer-test6-8 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,529] INFO Stopping connector test6 (org.apache.kafka.connect.runtime.Worker:387)
[2021-02-20 22:14:31,530] INFO Scheduled shutdown for WorkerConnector{id=test6} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2021-02-20 22:14:31,530] INFO Completed shutdown for WorkerConnector{id=test6} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2021-02-20 22:14:31,532] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:209)
[2021-02-20 22:14:31,534] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2021-02-20 22:14:31,534] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:14:31,534] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:14:31,535] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:14:31,536] INFO App info kafka.connect for 127.0.1.1:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:14:31,536] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:230)
[2021-02-20 22:14:31,539] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:120)
[2021-02-20 22:14:31,540] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2021-02-20 22:14:53,450] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2021-02-20 22:14:53,487] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../logs, -Dlog4j.configuration=file:./kafka_2.13-2.7.0/bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_281, 25.281-b09
	jvm.classpath = /home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/activation-1.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/argparse4j-0.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/audience-annotations-0.5.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-cli-1.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-lang3-3.8.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-api-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-basic-auth-extension-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-file-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-json-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-client-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-runtime-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-transforms-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-api-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-locator-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-utils-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-core-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-databind-2.10.5.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-paranamer-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-scala_2.13-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.inject-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.25.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.26.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.servlet-api-3.1.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jaxb-api-2.3.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-client-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-common-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-core-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-hk2-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-media-jaxb-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-server-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-client-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-continuation-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-http-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-io-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-security-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-server-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlet-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlets-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-util-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jopt-simple-5.0.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0-sources.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-clients-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-log4j-appender-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-raft-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-examples-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-scala_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-test-utils-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-tools-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/log4j-1.2.17.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/lz4-java-1.7.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/maven-artifact-3.6.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/metrics-core-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-buffer-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-codec-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-handler-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-resolver-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-epoll-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-unix-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/paranamer-2.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/plexus-utils-3.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/reflections-0.9.12.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/rocksdbjni-5.18.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-collection-compat_2.13-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-java8-compat_2.13-0.9.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-library-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-logging_2.13-3.9.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-reflect-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-api-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/snappy-java-1.1.7.7.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-jute-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zstd-jni-1.4.5-6.jar
	os.spec = Linux, amd64, 4.4.0-103-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-02-20 22:14:53,493] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2021-02-20 22:14:53,613] INFO Loading plugin from: /home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-02-20 22:14:59,991] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:14:59,995] INFO Added plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:14:59,997] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:14:59,999] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:00,000] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:00,001] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:00,002] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,409] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:15:05,410] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,411] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,412] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,412] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,418] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,419] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,419] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,420] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,421] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,421] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,422] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,423] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,424] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,425] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,426] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,427] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,427] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,428] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,430] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,430] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,431] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,432] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,433] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,433] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,434] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,435] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,436] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,437] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,438] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,439] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,440] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,441] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,441] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,442] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,443] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,444] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,445] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,446] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,446] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,447] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,448] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,449] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,450] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,451] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,452] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,453] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,454] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:15:05,459] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,461] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,462] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,463] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,464] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,465] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,466] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,467] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,468] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,469] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,469] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,470] INFO Added aliases 'RocksetSinkConnector' and 'RocksetSink' to plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,471] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,471] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,472] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,473] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,473] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,474] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,475] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,476] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,476] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,477] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,478] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,478] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,479] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,480] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,480] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,481] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,482] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,482] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,484] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,486] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,488] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,489] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,490] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,491] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,491] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,492] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,493] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:15:05,493] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,494] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,494] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:15:05,611] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [kafka-connect-rockset-1.2.0-jar-with-dependencies.jar]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:361)
[2021-02-20 22:15:05,618] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:15:05,637] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:15:06,107] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:06,107] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:06,108] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:06,108] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:06,109] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:06,110] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:06,110] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:06,113] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:06,114] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:06,115] INFO Kafka startTimeMs: 1613877306111 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:07,901] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:15:07,913] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:15:07,955] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:15:07,956] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:15:07,957] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:15:08,013] INFO Logging initialized @16365ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2021-02-20 22:15:08,278] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-02-20 22:15:08,279] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-02-20 22:15:08,317] INFO jetty-9.4.33.v20201020; built: 2020-10-20T23:39:24.803Z; git: 1be68755656cef678b79a2ef1c2ebbca99e25420; jvm 1.8.0_281-b09 (org.eclipse.jetty.server.Server:375)
[2021-02-20 22:15:08,458] INFO Started http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-02-20 22:15:08,459] INFO Started @16812ms (org.eclipse.jetty.server.Server:415)
[2021-02-20 22:15:08,569] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:15:08,570] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-02-20 22:15:08,571] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:15:08,573] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-02-20 22:15:08,574] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:15:08,576] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-02-20 22:15:08,618] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:15:08,622] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:15:08,651] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:08,651] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:08,652] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:08,653] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:08,653] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:08,654] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:08,654] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:15:08,655] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:08,656] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:08,656] INFO Kafka startTimeMs: 1613877308655 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:08,712] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:15:08,715] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:15:08,745] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:15:08,746] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:15:08,747] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:15:08,771] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:08,771] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:08,772] INFO Kafka startTimeMs: 1613877308770 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:09,375] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:15:09,382] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:15:09,422] INFO Kafka Connect standalone worker initialization took 15962ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2021-02-20 22:15:09,423] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-02-20 22:15:09,427] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-02-20 22:15:09,428] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2021-02-20 22:15:09,431] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-02-20 22:15:09,443] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2021-02-20 22:15:09,443] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-02-20 22:15:09,444] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-02-20 22:15:09,662] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-02-20 22:15:10,054] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-02-20 22:15:10,055] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-02-20 22:15:10,060] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2021-02-20 22:15:12,613] INFO Started o.e.j.s.ServletContextHandler@2b0b4d53{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2021-02-20 22:15:12,614] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-02-20 22:15:12,614] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-02-20 22:15:12,690] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2021-02-20 22:15:12,739] INFO Creating connector test6 of type rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2021-02-20 22:15:12,741] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:12,744] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:12,766] INFO Instantiated connector test6 with version 1.0 of type class rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2021-02-20 22:15:12,768] INFO Finished creating connector test6 (org.apache.kafka.connect.runtime.Worker:310)
[2021-02-20 22:15:12,774] INFO Starting RocksetSinkConnector (rockset.RocksetSinkConnector:22)
[2021-02-20 22:15:12,779] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:12,781] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:12,792] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:12,794] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:12,807] INFO Creating task test6-0 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:12,817] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:12,818] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:12,824] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:12,826] INFO Instantiated task test6-0 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:12,831] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:12,832] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:12,833] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-0 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:12,833] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-0 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:12,835] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:12,857] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:12,859] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:12,861] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:12,911] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,118] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,119] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,119] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,120] INFO Kafka startTimeMs: 1613877313119 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,167] INFO Creating task test6-1 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,171] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,173] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,174] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,175] INFO Instantiated task test6-1 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,175] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,176] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,177] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-1 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,177] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-1 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,179] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-1 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,179] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,183] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,184] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,186] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,188] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,190] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,195] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,237] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,238] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,238] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,239] INFO Kafka startTimeMs: 1613877313237 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,258] INFO Creating task test6-2 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,259] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,267] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,272] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,273] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,281] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,287] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,288] INFO Instantiated task test6-2 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,289] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,290] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,290] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-2 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,291] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-2 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,292] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-2 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,297] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,298] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,300] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,312] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,343] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,344] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,345] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,345] INFO Kafka startTimeMs: 1613877313344 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,364] INFO Creating task test6-3 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,370] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,371] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,385] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,386] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,390] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,397] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,397] INFO Instantiated task test6-3 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,399] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,400] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,400] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-3 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,401] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-3 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,403] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-3 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,409] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,417] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,420] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,425] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,462] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,464] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,465] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,465] INFO Kafka startTimeMs: 1613877313463 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,487] INFO Creating task test6-4 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,488] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,494] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,497] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,500] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,502] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,507] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,512] INFO Instantiated task test6-4 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,514] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,515] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,516] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-4 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,517] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-4 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,518] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-4 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,528] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,530] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,532] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,535] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,565] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,567] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,567] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,568] INFO Kafka startTimeMs: 1613877313566 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,579] INFO Creating task test6-5 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,583] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,589] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,590] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,591] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,592] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,596] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,597] INFO Instantiated task test6-5 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,598] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,600] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,601] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-5 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,603] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-5 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,604] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-5 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,609] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,611] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,613] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,616] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,645] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,646] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,646] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,647] INFO Kafka startTimeMs: 1613877313646 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,663] INFO Creating task test6-6 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,663] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,665] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,668] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,669] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,677] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,679] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,681] INFO Instantiated task test6-6 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,682] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,684] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,684] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-6 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,685] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-6 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,686] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-6 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,691] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,693] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,696] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,700] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,728] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,729] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,730] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,730] INFO Kafka startTimeMs: 1613877313729 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,742] INFO Creating task test6-7 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,743] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,748] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,749] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,748] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,750] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,754] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,755] INFO Instantiated task test6-7 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,756] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,759] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,760] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-7 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,761] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-7 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,761] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-7 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,766] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,771] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,774] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,776] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,803] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,804] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,804] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,805] INFO Kafka startTimeMs: 1613877313804 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,832] INFO Creating task test6-8 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,842] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,845] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,852] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,853] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,854] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,856] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,858] INFO Instantiated task test6-8 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,859] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,860] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,860] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-8 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,861] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-8 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,861] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-8 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,868] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,873] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,881] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,888] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,916] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,917] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,918] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,918] INFO Kafka startTimeMs: 1613877313917 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,930] INFO Creating task test6-9 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:15:13,931] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,933] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:15:13,934] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,935] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:13,935] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,937] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:15:13,937] INFO Instantiated task test6-9 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:15:13,938] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,939] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:15:13,939] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test6-9 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:15:13,940] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test6-9 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:15:13,940] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test6-9 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:15:13,944] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:15:13,945] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:15:13,946] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test6
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:15:13,949] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test6-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test6
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:15:13,970] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:15:13,971] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:15:13,971] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:15:13,972] INFO Kafka startTimeMs: 1613877313971 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:15:13,986] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:15:13,989] INFO Created connector test6 (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2021-02-20 22:15:13,989] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:15:13,991] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:15:15,496] INFO WorkerSinkTask{id=test6-6} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,496] INFO WorkerSinkTask{id=test6-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,496] INFO WorkerSinkTask{id=test6-2} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,502] INFO WorkerSinkTask{id=test6-7} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,504] INFO WorkerSinkTask{id=test6-1} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,496] INFO WorkerSinkTask{id=test6-8} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,497] INFO WorkerSinkTask{id=test6-3} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,499] INFO WorkerSinkTask{id=test6-5} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,499] INFO WorkerSinkTask{id=test6-4} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,499] INFO WorkerSinkTask{id=test6-9} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:15:15,682] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,684] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,684] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,682] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,682] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,703] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,707] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,709] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,712] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,713] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,731] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,734] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,735] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,739] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,709] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:15:15,744] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,741] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,740] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,713] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,752] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:15:15,771] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,778] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,778] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,780] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,781] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,782] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,783] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,780] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,781] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,784] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,866] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,883] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,884] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,884] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,891] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully joined group with generation Generation{generationId=4, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,888] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,894] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,896] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,894] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,900] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,900] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,911] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Finished assignment for group at generation 4: {connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9=Assignment(partitions=[test2-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:15:15,938] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=4, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:781)
[2021-02-20 22:15:15,941] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:472)
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
[2021-02-20 22:15:15,949] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:15:15,957] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,958] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,959] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-0-d9ae6daa-6ca1-4b6b-9931-9baa4d3bea19', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,965] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-1-9b718aa3-8c6e-418a-ab66-f8885a564ff0', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,959] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,959] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,958] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,957] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,957] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,975] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:15:15,989] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Finished assignment for group at generation 5: {connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760=Assignment(partitions=[]), connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d=Assignment(partitions=[]), connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f=Assignment(partitions=[]), connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a=Assignment(partitions=[]), connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837=Assignment(partitions=[]), connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5=Assignment(partitions=[]), connector-consumer-test6-1-9b718aa3-8c6e-418a-ab66-f8885a564ff0=Assignment(partitions=[]), connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9=Assignment(partitions=[]), connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4=Assignment(partitions=[]), connector-consumer-test6-0-d9ae6daa-6ca1-4b6b-9931-9baa4d3bea19=Assignment(partitions=[test2-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:15:16,000] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,001] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,001] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,002] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,000] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,000] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,004] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-1-9b718aa3-8c6e-418a-ab66-f8885a564ff0', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,004] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,004] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,002] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,002] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-0-d9ae6daa-6ca1-4b6b-9931-9baa4d3bea19', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,002] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,002] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,010] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[test2-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,011] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,012] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,008] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,007] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,006] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,006] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,017] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,005] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,034] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,005] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:15:16,038] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,039] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,031] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Adding newly assigned partitions: test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,016] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,011] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:15:16,042] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:15:16,152] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Setting offset for partition test2-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-02-20 22:16:23,167] INFO WorkerSinkTask{id=test6-0} Committing offsets asynchronously using sequence number 7: {test2-0=OffsetAndMetadata{offset=4, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:17:33,188] INFO WorkerSinkTask{id=test6-0} Committing offsets asynchronously using sequence number 14: {test2-0=OffsetAndMetadata{offset=5, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:19:13,202] INFO WorkerSinkTask{id=test6-0} Committing offsets asynchronously using sequence number 24: {test2-0=OffsetAndMetadata{offset=6, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:20:43,211] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:20:43,217] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 6 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:20:43,218] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 33: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:20:53,211] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:20:53,216] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 6 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:20:53,216] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 34: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:03,211] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:03,216] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 6 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:21:03,216] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 35: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:13,212] ERROR WorkerSinkTask{id=test6-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:13,216] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Seeking to offset 6 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:21:13,217] ERROR WorkerSinkTask{id=test6-0} Commit of offsets threw an unexpected exception for sequence number 36: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:22,341] ERROR WorkerSinkTask{id=test6-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream  (org.apache.kafka.connect.runtime.WorkerSinkTask:612)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.lambda$submitForProcessing$3(RocksetSinkTask.java:97)
	at java.util.HashMap.forEach(HashMap.java:1289)
	at rockset.RocksetSinkTask.submitForProcessing(RocksetSinkTask.java:95)
	at rockset.RocksetSinkTask.put(RocksetSinkTask.java:80)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 15 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:22,348] INFO WorkerSinkTask{id=test6-0} Committing offsets synchronously using sequence number 37: {test2-0=OffsetAndMetadata{offset=7, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:338)
[2021-02-20 22:21:22,354] ERROR WorkerSinkTask{id=test6-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:614)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.lambda$submitForProcessing$3(RocksetSinkTask.java:97)
	at java.util.HashMap.forEach(HashMap.java:1289)
	at rockset.RocksetSinkTask.submitForProcessing(RocksetSinkTask.java:95)
	at rockset.RocksetSinkTask.put(RocksetSinkTask.java:80)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	... 10 more
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 15 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:22,357] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:21:22,358] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:21:22,364] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Revoke previously assigned partitions test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:22,365] INFO [Consumer clientId=connector-consumer-test6-0, groupId=connect-test6] Member connector-consumer-test6-0-d9ae6daa-6ca1-4b6b-9931-9baa4d3bea19 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:21:22,374] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:21:22,374] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:21:22,375] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:21:22,385] INFO App info kafka.consumer for connector-consumer-test6-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:21:25,051] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,051] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,054] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,054] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,055] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,055] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,066] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,067] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,068] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,077] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,078] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,079] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,080] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,081] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,081] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,082] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,081] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,084] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,084] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,085] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,087] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,087] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:21:25,089] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,089] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,086] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,088] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:25,091] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:21:25,096] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,097] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-1-9b718aa3-8c6e-418a-ab66-f8885a564ff0', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,097] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,097] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,096] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,096] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,098] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,097] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,097] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:21:25,103] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Finished assignment for group at generation 6: {connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760=Assignment(partitions=[]), connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d=Assignment(partitions=[]), connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f=Assignment(partitions=[]), connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a=Assignment(partitions=[]), connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837=Assignment(partitions=[]), connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5=Assignment(partitions=[]), connector-consumer-test6-1-9b718aa3-8c6e-418a-ab66-f8885a564ff0=Assignment(partitions=[test2-0]), connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9=Assignment(partitions=[]), connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:21:25,109] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,111] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,113] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,113] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,114] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-1-9b718aa3-8c6e-418a-ab66-f8885a564ff0', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,110] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,109] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,116] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,117] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,116] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[test2-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,115] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,113] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,112] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,112] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,120] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,120] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,119] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,118] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Adding newly assigned partitions: test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,117] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,123] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,122] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,121] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,127] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,124] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:21:25,130] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:21:25,131] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,124] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:21:25,133] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Setting offset for partition test2-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-02-20 22:21:33,274] ERROR WorkerSinkTask{id=test6-1} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:33,279] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Seeking to offset 7 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:21:33,279] ERROR WorkerSinkTask{id=test6-1} Commit of offsets threw an unexpected exception for sequence number 1: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:43,276] ERROR WorkerSinkTask{id=test6-1} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:43,281] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Seeking to offset 7 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:21:43,281] ERROR WorkerSinkTask{id=test6-1} Commit of offsets threw an unexpected exception for sequence number 2: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:53,277] ERROR WorkerSinkTask{id=test6-1} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:53,281] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Seeking to offset 7 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:21:53,281] ERROR WorkerSinkTask{id=test6-1} Commit of offsets threw an unexpected exception for sequence number 3: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:59,527] ERROR WorkerSinkTask{id=test6-1} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream  (org.apache.kafka.connect.runtime.WorkerSinkTask:612)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.lambda$submitForProcessing$3(RocksetSinkTask.java:97)
	at java.util.HashMap.forEach(HashMap.java:1289)
	at rockset.RocksetSinkTask.submitForProcessing(RocksetSinkTask.java:95)
	at rockset.RocksetSinkTask.put(RocksetSinkTask.java:80)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 15 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:59,532] INFO WorkerSinkTask{id=test6-1} Committing offsets synchronously using sequence number 4: {test2-0=OffsetAndMetadata{offset=8, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:338)
[2021-02-20 22:21:59,535] ERROR WorkerSinkTask{id=test6-1} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:614)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.lambda$submitForProcessing$3(RocksetSinkTask.java:97)
	at java.util.HashMap.forEach(HashMap.java:1289)
	at rockset.RocksetSinkTask.submitForProcessing(RocksetSinkTask.java:95)
	at rockset.RocksetSinkTask.put(RocksetSinkTask.java:80)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	... 10 more
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 15 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: was expecting closing quote for a string value
 at [Source: (String)"{"id": 20, "first_name": "Name}"; line: 1, column: 63]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:618)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString2(ReaderBasedJsonParser.java:2037)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._finishString(ReaderBasedJsonParser.java:2024)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.getText(ReaderBasedJsonParser.java:278)
	at com.fasterxml.jackson.databind.deser.std.UntypedObjectDeserializer$Vanilla.deserialize(UntypedObjectDeserializer.java:672)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:527)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:21:59,537] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:21:59,538] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:21:59,541] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Revoke previously assigned partitions test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:21:59,541] INFO [Consumer clientId=connector-consumer-test6-1, groupId=connect-test6] Member connector-consumer-test6-1-9b718aa3-8c6e-418a-ab66-f8885a564ff0 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:21:59,548] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:21:59,548] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:21:59,549] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:21:59,559] INFO App info kafka.consumer for connector-consumer-test6-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:22:01,105] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,106] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,107] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,108] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,105] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,109] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,109] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,108] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,108] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,108] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,107] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,107] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:22:01,113] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,112] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,111] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,111] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,110] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,110] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,117] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,115] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,115] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,114] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:22:01,120] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,117] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:22:01,126] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,128] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,127] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,127] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,127] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,126] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,126] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,126] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:22:01,131] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Finished assignment for group at generation 7: {connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760=Assignment(partitions=[]), connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d=Assignment(partitions=[]), connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f=Assignment(partitions=[test2-0]), connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a=Assignment(partitions=[]), connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837=Assignment(partitions=[]), connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5=Assignment(partitions=[]), connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9=Assignment(partitions=[]), connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:22:01,139] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,140] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,140] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,139] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,139] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,142] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,141] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,141] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,140] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,140] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,140] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,140] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:22:01,147] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,146] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,145] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,144] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,144] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,143] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,143] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[test2-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,149] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,148] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,148] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:22:01,151] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Adding newly assigned partitions: test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,153] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:22:01,156] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Setting offset for partition test2-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-02-20 22:22:03,388] INFO WorkerSinkTask{id=test6-2} Committing offsets asynchronously using sequence number 1: {test2-0=OffsetAndMetadata{offset=9, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:23:03,394] INFO WorkerSinkTask{id=test6-2} Committing offsets asynchronously using sequence number 7: {test2-0=OffsetAndMetadata{offset=10, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:27:33,746] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2021-02-20 22:27:33,748] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2021-02-20 22:27:33,764] INFO Stopped http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2021-02-20 22:27:33,766] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2021-02-20 22:27:33,772] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2021-02-20 22:27:33,773] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:102)
[2021-02-20 22:27:33,777] INFO Stopping task test6-0 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,778] INFO Stopping task test6-1 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,780] INFO Stopping task test6-2 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,781] INFO Stopping task test6-3 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,782] INFO Stopping task test6-4 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,783] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,784] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,784] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,784] INFO Stopping task test6-5 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,784] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,787] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,786] INFO Stopping task test6-6 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,786] INFO [Consumer clientId=connector-consumer-test6-3, groupId=connect-test6] Member connector-consumer-test6-3-3abf1122-3195-4938-bf46-28e33e083760 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,784] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,790] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,790] INFO Stopping task test6-7 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,788] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,793] INFO Stopping task test6-8 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,793] INFO [Consumer clientId=connector-consumer-test6-5, groupId=connect-test6] Member connector-consumer-test6-5-a65ed99d-ab44-45d9-98e7-460e8d422ed5 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,787] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,794] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,794] INFO Stopping task test6-9 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:27:33,793] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,791] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,791] INFO [Consumer clientId=connector-consumer-test6-4, groupId=connect-test6] Member connector-consumer-test6-4-a1df1b7d-3bde-4b6a-9299-74f67fc912b9 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,798] INFO [Consumer clientId=connector-consumer-test6-6, groupId=connect-test6] Member connector-consumer-test6-6-9f537a8b-a7e0-45c8-b56d-43be454fc8f4 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,797] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:27:33,801] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,797] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,795] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Revoke previously assigned partitions test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:27:33,803] INFO [Consumer clientId=connector-consumer-test6-2, groupId=connect-test6] Member connector-consumer-test6-2-2be2a816-a394-4723-bb9d-b8772deb6c2f sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,795] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:27:33,795] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,806] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,807] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,808] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,805] INFO [Consumer clientId=connector-consumer-test6-8, groupId=connect-test6] Member connector-consumer-test6-8-92149ded-7aba-42ec-8a21-734e5ba47837 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,804] INFO [Consumer clientId=connector-consumer-test6-9, groupId=connect-test6] Member connector-consumer-test6-9-c2aecb16-8829-4270-87c5-93faf6ce6f7a sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,804] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,812] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,803] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,803] INFO [Consumer clientId=connector-consumer-test6-7, groupId=connect-test6] Member connector-consumer-test6-7-d00365ab-9a9b-4d00-a51d-8deec7e0f29d sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:27:33,799] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,818] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,813] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,819] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,813] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,813] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,808] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,807] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,820] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,820] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,823] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,823] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,832] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,836] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,840] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,841] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,845] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,906] INFO App info kafka.consumer for connector-consumer-test6-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,921] INFO App info kafka.consumer for connector-consumer-test6-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,924] INFO App info kafka.consumer for connector-consumer-test6-8 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,927] INFO App info kafka.consumer for connector-consumer-test6-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,932] INFO App info kafka.consumer for connector-consumer-test6-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,936] INFO App info kafka.consumer for connector-consumer-test6-9 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,938] INFO App info kafka.consumer for connector-consumer-test6-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,940] INFO App info kafka.consumer for connector-consumer-test6-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,984] INFO Stopping connector test6 (org.apache.kafka.connect.runtime.Worker:387)
[2021-02-20 22:27:33,985] INFO Scheduled shutdown for WorkerConnector{id=test6} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2021-02-20 22:27:33,986] INFO Completed shutdown for WorkerConnector{id=test6} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2021-02-20 22:27:33,988] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:209)
[2021-02-20 22:27:33,990] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2021-02-20 22:27:33,990] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:33,990] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:33,992] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:33,993] INFO App info kafka.connect for 127.0.1.1:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:33,993] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:230)
[2021-02-20 22:27:33,997] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:120)
[2021-02-20 22:27:33,997] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2021-02-20 22:27:43,474] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2021-02-20 22:27:43,511] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../logs, -Dlog4j.configuration=file:./kafka_2.13-2.7.0/bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_281, 25.281-b09
	jvm.classpath = /home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/activation-1.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/argparse4j-0.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/audience-annotations-0.5.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-cli-1.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-lang3-3.8.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-api-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-basic-auth-extension-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-file-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-json-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-client-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-runtime-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-transforms-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-api-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-locator-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-utils-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-core-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-databind-2.10.5.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-paranamer-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-scala_2.13-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.inject-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.25.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.26.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.servlet-api-3.1.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jaxb-api-2.3.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-client-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-common-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-core-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-hk2-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-media-jaxb-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-server-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-client-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-continuation-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-http-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-io-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-security-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-server-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlet-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlets-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-util-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jopt-simple-5.0.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0-sources.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-clients-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-log4j-appender-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-raft-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-examples-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-scala_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-test-utils-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-tools-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/log4j-1.2.17.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/lz4-java-1.7.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/maven-artifact-3.6.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/metrics-core-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-buffer-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-codec-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-handler-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-resolver-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-epoll-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-unix-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/paranamer-2.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/plexus-utils-3.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/reflections-0.9.12.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/rocksdbjni-5.18.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-collection-compat_2.13-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-java8-compat_2.13-0.9.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-library-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-logging_2.13-3.9.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-reflect-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-api-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/snappy-java-1.1.7.7.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-jute-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zstd-jni-1.4.5-6.jar
	os.spec = Linux, amd64, 4.4.0-103-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-02-20 22:27:43,516] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2021-02-20 22:27:43,628] INFO Loading plugin from: /home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-02-20 22:27:50,416] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:27:50,419] INFO Added plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:50,420] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:50,421] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:50,422] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:50,422] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:50,423] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,491] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:27:55,492] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,492] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,493] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,494] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,500] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,501] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,501] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,502] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,503] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,503] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,504] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,504] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,505] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,506] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,507] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,508] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,508] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,509] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,509] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,510] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,511] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,512] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,513] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,513] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,514] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,515] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,516] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,516] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,517] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,518] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,518] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,519] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,519] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,520] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,520] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,521] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,521] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,522] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,523] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,523] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,524] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,524] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,525] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,526] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,527] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,527] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,528] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:27:55,532] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,533] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,534] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,535] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,536] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,537] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,538] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,539] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,540] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,541] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,542] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,543] INFO Added aliases 'RocksetSinkConnector' and 'RocksetSink' to plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,544] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,545] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,545] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,546] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,547] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,548] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,549] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,550] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,550] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,551] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,552] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,553] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,554] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,555] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,556] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,557] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,558] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,558] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,561] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,564] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,567] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,569] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,571] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,572] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,572] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,573] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,574] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:27:55,575] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,576] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,576] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:27:55,677] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [kafka-connect-rockset-1.2.0-jar-with-dependencies.jar]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:361)
[2021-02-20 22:27:55,681] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:27:55,700] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:27:56,122] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:56,122] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:56,123] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:56,123] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:56,124] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:56,124] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:56,125] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:56,127] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:27:56,127] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:27:56,128] INFO Kafka startTimeMs: 1613878076125 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:27:57,950] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:27:57,962] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:58,003] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:58,004] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:58,004] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:58,081] INFO Logging initialized @16418ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2021-02-20 22:27:58,351] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-02-20 22:27:58,352] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-02-20 22:27:58,395] INFO jetty-9.4.33.v20201020; built: 2020-10-20T23:39:24.803Z; git: 1be68755656cef678b79a2ef1c2ebbca99e25420; jvm 1.8.0_281-b09 (org.eclipse.jetty.server.Server:375)
[2021-02-20 22:27:58,545] INFO Started http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-02-20 22:27:58,546] INFO Started @16883ms (org.eclipse.jetty.server.Server:415)
[2021-02-20 22:27:58,654] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:27:58,655] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-02-20 22:27:58,656] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:27:58,657] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-02-20 22:27:58,658] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:27:58,660] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-02-20 22:27:58,706] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:27:58,709] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:27:58,737] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:58,738] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:58,738] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:58,739] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:58,739] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:58,740] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:58,740] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:27:58,741] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:27:58,741] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:27:58,742] INFO Kafka startTimeMs: 1613878078741 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:27:58,799] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:27:58,802] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:27:58,828] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:27:58,830] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:27:58,831] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:27:58,851] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:27:58,852] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:27:58,852] INFO Kafka startTimeMs: 1613878078851 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:27:59,441] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:27:59,447] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:27:59,487] INFO Kafka Connect standalone worker initialization took 16003ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2021-02-20 22:27:59,488] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-02-20 22:27:59,492] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-02-20 22:27:59,493] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2021-02-20 22:27:59,495] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-02-20 22:27:59,508] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2021-02-20 22:27:59,508] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-02-20 22:27:59,509] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-02-20 22:27:59,702] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-02-20 22:28:00,064] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-02-20 22:28:00,065] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-02-20 22:28:00,070] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2021-02-20 22:28:02,748] INFO Started o.e.j.s.ServletContextHandler@2b0b4d53{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2021-02-20 22:28:02,749] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-02-20 22:28:02,749] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-02-20 22:28:02,826] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2021-02-20 22:28:02,879] INFO Creating connector test7 of type rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2021-02-20 22:28:02,883] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:02,885] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:02,907] INFO Instantiated connector test7 with version 1.0 of type class rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2021-02-20 22:28:02,909] INFO Finished creating connector test7 (org.apache.kafka.connect.runtime.Worker:310)
[2021-02-20 22:28:02,914] INFO Starting RocksetSinkConnector (rockset.RocksetSinkConnector:22)
[2021-02-20 22:28:02,919] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:02,920] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:02,928] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:02,929] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:02,940] INFO Creating task test7-0 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:02,950] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:02,951] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:02,956] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:02,958] INFO Instantiated task test7-0 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:02,964] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:02,965] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:02,966] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-0 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:02,966] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-0 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:02,968] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:02,990] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:02,992] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:02,993] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,042] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,239] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,239] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,240] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,240] INFO Kafka startTimeMs: 1613878083239 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,282] INFO Creating task test7-1 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,286] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,287] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,288] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,288] INFO Instantiated task test7-1 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,289] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,290] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,290] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-1 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,291] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-1 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,291] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-1 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,294] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,298] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,298] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,300] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,300] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,302] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,306] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,343] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,344] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,345] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,345] INFO Kafka startTimeMs: 1613878083344 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,358] INFO Creating task test7-2 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,361] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,367] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,368] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,369] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,375] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,377] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,378] INFO Instantiated task test7-2 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,379] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,383] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,384] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-2 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,385] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-2 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,386] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-2 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,401] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,403] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,405] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,407] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,440] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,441] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,442] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,442] INFO Kafka startTimeMs: 1613878083441 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,453] INFO Creating task test7-3 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,456] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,460] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,461] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,463] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,463] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,467] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,469] INFO Instantiated task test7-3 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,470] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,472] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,472] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-3 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,473] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-3 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,474] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-3 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,482] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,485] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,487] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,499] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,530] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,531] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,532] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,532] INFO Kafka startTimeMs: 1613878083531 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,552] INFO Creating task test7-4 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,556] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,564] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,569] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,578] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,583] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,584] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,591] INFO Instantiated task test7-4 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,592] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,595] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,596] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-4 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,597] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-4 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,598] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-4 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,604] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,606] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,608] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,612] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,641] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,642] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,643] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,644] INFO Kafka startTimeMs: 1613878083642 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,658] INFO Creating task test7-5 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,665] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,668] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,670] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,670] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,671] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,679] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,687] INFO Instantiated task test7-5 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,688] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,689] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,690] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-5 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,691] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-5 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,691] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-5 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,697] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,699] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,703] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,707] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,741] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,742] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,743] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,744] INFO Kafka startTimeMs: 1613878083742 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,757] INFO Creating task test7-6 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,758] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,762] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,763] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,764] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,764] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,767] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,770] INFO Instantiated task test7-6 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,771] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,772] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,772] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-6 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,775] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-6 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,776] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-6 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,783] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,787] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,789] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,793] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,824] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,826] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,827] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,827] INFO Kafka startTimeMs: 1613878083826 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,842] INFO Creating task test7-7 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,843] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,847] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,848] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,849] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,849] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,851] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,852] INFO Instantiated task test7-7 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,853] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,856] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,857] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-7 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,858] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-7 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,859] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-7 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,864] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,867] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,872] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,876] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,904] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,905] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,906] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,906] INFO Kafka startTimeMs: 1613878083905 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,917] INFO Creating task test7-8 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:03,924] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:03,925] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:03,929] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:03,930] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:03,931] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,932] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:03,933] INFO Instantiated task test7-8 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:03,934] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,935] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:03,936] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-8 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:03,936] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-8 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:03,937] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-8 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:03,943] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:03,945] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:03,948] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:03,955] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:03,985] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:03,986] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:03,987] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:03,988] INFO Kafka startTimeMs: 1613878083986 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:03,998] INFO Creating task test7-9 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:28:04,003] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:04,007] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:04,008] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:04,010] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:28:04,011] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:04,012] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:28:04,012] INFO Instantiated task test7-9 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:28:04,013] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:04,013] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:28:04,014] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-9 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:28:04,014] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-9 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:28:04,015] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-9 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:28:04,020] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:28:04,023] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:28:04,025] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:28:04,029] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:28:04,051] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:28:04,052] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:28:04,052] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:28:04,053] INFO Kafka startTimeMs: 1613878084051 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:28:04,067] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:28:04,070] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://aFLL2ceDsoe927ZZoddskv13dWGU06DCbgb9ddcjerzldebwBonuHoQMRWD2yqOv@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:28:04,070] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:28:04,072] INFO Created connector test7 (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2021-02-20 22:28:05,467] INFO WorkerSinkTask{id=test7-2} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,467] INFO WorkerSinkTask{id=test7-1} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,468] INFO WorkerSinkTask{id=test7-7} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,469] INFO WorkerSinkTask{id=test7-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,467] INFO WorkerSinkTask{id=test7-6} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,467] INFO WorkerSinkTask{id=test7-5} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,468] INFO WorkerSinkTask{id=test7-4} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,467] INFO WorkerSinkTask{id=test7-9} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,469] INFO WorkerSinkTask{id=test7-3} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,467] INFO WorkerSinkTask{id=test7-8} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:28:05,678] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,680] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,682] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,684] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,682] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,681] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,688] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,690] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,692] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,681] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,688] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,688] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,699] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,701] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,688] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,688] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:28:05,687] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,685] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,709] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,706] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,712] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,723] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,722] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,721] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,720] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,712] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,725] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,724] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:28:05,730] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,735] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,829] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,837] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,838] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,852] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,841] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,841] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,857] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,860] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,856] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,868] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,856] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,866] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,865] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,890] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:781)
[2021-02-20 22:28:05,890] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:781)
[2021-02-20 22:28:05,896] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:472)
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
[2021-02-20 22:28:05,898] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Finished assignment for group at generation 1: {connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881=Assignment(partitions=[test2-0]), connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1=Assignment(partitions=[]), connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:28:05,896] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:472)
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
[2021-02-20 22:28:05,901] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,903] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,928] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:781)
[2021-02-20 22:28:05,929] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:472)
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
[2021-02-20 22:28:05,930] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:28:05,938] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,939] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-4-bdd5d075-cd3a-4f09-a099-cb41ae271fb5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,938] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-3-48815871-48d5-470e-abea-0bad13b815c6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,941] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,940] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-6-eced6917-746d-4e9f-85af-7a5ddb14b23d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,939] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-0-b2a06e94-89de-4b0e-9e25-e5203b1d486e', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,939] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,939] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-2-68545755-a247-4a55-bd63-b333414ed3dd', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,951] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-7-e1b6fa14-5b59-4a04-8ab3-b69b2b167c5c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,963] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Finished assignment for group at generation 2: {connector-consumer-test7-3-48815871-48d5-470e-abea-0bad13b815c6=Assignment(partitions=[]), connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881=Assignment(partitions=[]), connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1=Assignment(partitions=[]), connector-consumer-test7-8-cad8d27a-5de8-4377-99cd-1ee95c1b95b4=Assignment(partitions=[]), connector-consumer-test7-6-eced6917-746d-4e9f-85af-7a5ddb14b23d=Assignment(partitions=[]), connector-consumer-test7-0-b2a06e94-89de-4b0e-9e25-e5203b1d486e=Assignment(partitions=[test2-0]), connector-consumer-test7-4-bdd5d075-cd3a-4f09-a099-cb41ae271fb5=Assignment(partitions=[]), connector-consumer-test7-7-e1b6fa14-5b59-4a04-8ab3-b69b2b167c5c=Assignment(partitions=[]), connector-consumer-test7-2-68545755-a247-4a55-bd63-b333414ed3dd=Assignment(partitions=[]), connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:28:05,948] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-test7-8-cad8d27a-5de8-4377-99cd-1ee95c1b95b4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:28:05,981] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-3-48815871-48d5-470e-abea-0bad13b815c6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,981] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,981] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-4-bdd5d075-cd3a-4f09-a099-cb41ae271fb5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,981] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-8-cad8d27a-5de8-4377-99cd-1ee95c1b95b4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,981] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-2-68545755-a247-4a55-bd63-b333414ed3dd', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,989] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:05,990] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:05,991] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:05,981] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-0-b2a06e94-89de-4b0e-9e25-e5203b1d486e', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,991] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-6-eced6917-746d-4e9f-85af-7a5ddb14b23d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,997] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[test2-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:05,991] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,991] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:05,989] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:05,987] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:05,984] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:05,983] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:05,983] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-test7-7-e1b6fa14-5b59-4a04-8ab3-b69b2b167c5c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:28:06,003] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:06,003] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:06,002] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:06,002] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:06,011] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:05,997] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:06,013] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:06,006] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:06,005] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:28:06,017] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:06,016] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:06,020] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Adding newly assigned partitions: test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:28:06,042] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Found no committed offset for partition test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2021-02-20 22:28:06,095] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Resetting offset for partition test2-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2021-02-20 22:28:13,268] ERROR WorkerSinkTask{id=test7-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:13,273] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Seeking to offset 0 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:28:13,274] ERROR WorkerSinkTask{id=test7-0} Commit of offsets threw an unexpected exception for sequence number 1: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:23,269] ERROR WorkerSinkTask{id=test7-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:23,273] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Seeking to offset 0 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:28:23,274] ERROR WorkerSinkTask{id=test7-0} Commit of offsets threw an unexpected exception for sequence number 2: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:33,270] ERROR WorkerSinkTask{id=test7-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:33,277] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Seeking to offset 0 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:28:33,278] ERROR WorkerSinkTask{id=test7-0} Commit of offsets threw an unexpected exception for sequence number 3: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:43,271] ERROR WorkerSinkTask{id=test7-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:43,278] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Seeking to offset 0 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:28:43,279] ERROR WorkerSinkTask{id=test7-0} Commit of offsets threw an unexpected exception for sequence number 4: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:53,271] ERROR WorkerSinkTask{id=test7-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:28:53,276] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Seeking to offset 0 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:28:53,277] ERROR WorkerSinkTask{id=test7-0} Commit of offsets threw an unexpected exception for sequence number 5: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:29:03,273] ERROR WorkerSinkTask{id=test7-0} Offset commit failed, rewinding to last committed offsets (org.apache.kafka.connect.runtime.WorkerSinkTask:398)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:29:03,278] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Seeking to offset 0 for partition test2-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1587)
[2021-02-20 22:29:03,279] ERROR WorkerSinkTask{id=test7-0} Commit of offsets threw an unexpected exception for sequence number 6: null (org.apache.kafka.connect.runtime.WorkerSinkTask:267)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.flush(RocksetSinkTask.java:170)
	at org.apache.kafka.connect.sink.SinkTask.preCommit(SinkTask.java:125)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:392)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:29:07,737] ERROR WorkerSinkTask{id=test7-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream  (org.apache.kafka.connect.runtime.WorkerSinkTask:612)
java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.lambda$submitForProcessing$3(RocksetSinkTask.java:97)
	at java.util.HashMap.forEach(HashMap.java:1289)
	at rockset.RocksetSinkTask.submitForProcessing(RocksetSinkTask.java:95)
	at rockset.RocksetSinkTask.put(RocksetSinkTask.java:80)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 15 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:29:07,743] INFO WorkerSinkTask{id=test7-0} Committing offsets synchronously using sequence number 7: {test2-0=OffsetAndMetadata{offset=10, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:338)
[2021-02-20 22:29:07,755] ERROR WorkerSinkTask{id=test7-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:614)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Unable to write document for topic: test2, partition: 0, in Rockset, cause: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:131)
	at rockset.RocksetSinkTask.lambda$submitForProcessing$3(RocksetSinkTask.java:97)
	at java.util.HashMap.forEach(HashMap.java:1289)
	at rockset.RocksetSinkTask.submitForProcessing(RocksetSinkTask.java:95)
	at rockset.RocksetSinkTask.put(RocksetSinkTask.java:80)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	... 10 more
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at rockset.RocksetSinkTask.checkForFailures(RocksetSinkTask.java:122)
	... 15 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Invalid JSON encountered in stream 
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:103)
	at rockset.RocksetSinkTask.addWithRetries(RocksetSinkTask.java:143)
	at rockset.RocksetSinkTask.lambda$null$2(RocksetSinkTask.java:99)
	at rockset.utils.BlockingExecutor.lambda$submit$0(BlockingExecutor.java:27)
	... 5 more
Caused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `java.util.LinkedHashMap` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('id')
 at [Source: (String)""id": 20, "first_name": "Name"}"; line: 1, column: 1]
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)
	at com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator._createFromStringFallbacks(ValueInstantiator.java:371)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:323)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:357)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4013)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3023)
	at rockset.RocksetRequestWrapper.toMap(RocksetRequestWrapper.java:149)
	at rockset.RocksetRequestWrapper.addDoc(RocksetRequestWrapper.java:93)
	... 8 more
[2021-02-20 22:29:07,757] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:07,758] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:07,762] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Revoke previously assigned partitions test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:07,763] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Member connector-consumer-test7-0-b2a06e94-89de-4b0e-9e25-e5203b1d486e sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:07,772] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:07,773] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:07,773] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:07,784] INFO App info kafka.consumer for connector-consumer-test7-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:08,956] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,956] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,959] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,959] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,959] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,961] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,960] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,962] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,961] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,961] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,967] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,968] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,969] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,968] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,972] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,976] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,977] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,977] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,977] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,978] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,979] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,980] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,981] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,982] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,984] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1110)
[2021-02-20 22:29:08,985] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:08,985] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:29:08,992] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-3-48815871-48d5-470e-abea-0bad13b815c6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,994] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,994] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-6-eced6917-746d-4e9f-85af-7a5ddb14b23d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,993] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,999] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Finished assignment for group at generation 3: {connector-consumer-test7-3-48815871-48d5-470e-abea-0bad13b815c6=Assignment(partitions=[]), connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881=Assignment(partitions=[test2-0]), connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1=Assignment(partitions=[]), connector-consumer-test7-8-cad8d27a-5de8-4377-99cd-1ee95c1b95b4=Assignment(partitions=[]), connector-consumer-test7-6-eced6917-746d-4e9f-85af-7a5ddb14b23d=Assignment(partitions=[]), connector-consumer-test7-4-bdd5d075-cd3a-4f09-a099-cb41ae271fb5=Assignment(partitions=[]), connector-consumer-test7-7-e1b6fa14-5b59-4a04-8ab3-b69b2b167c5c=Assignment(partitions=[]), connector-consumer-test7-2-68545755-a247-4a55-bd63-b333414ed3dd=Assignment(partitions=[]), connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:29:08,996] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-2-68545755-a247-4a55-bd63-b333414ed3dd', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,996] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-8-cad8d27a-5de8-4377-99cd-1ee95c1b95b4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,995] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-7-e1b6fa14-5b59-4a04-8ab3-b69b2b167c5c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,995] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-4-bdd5d075-cd3a-4f09-a099-cb41ae271fb5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:08,994] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:29:09,007] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-3-48815871-48d5-470e-abea-0bad13b815c6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,008] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-7-e1b6fa14-5b59-4a04-8ab3-b69b2b167c5c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,007] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-6-eced6917-746d-4e9f-85af-7a5ddb14b23d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,010] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,009] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-8-cad8d27a-5de8-4377-99cd-1ee95c1b95b4', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,009] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,008] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,008] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-2-68545755-a247-4a55-bd63-b333414ed3dd', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,013] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,013] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,012] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,011] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,010] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,010] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,010] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-test7-4-bdd5d075-cd3a-4f09-a099-cb41ae271fb5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:29:09,010] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,019] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,019] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,018] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[test2-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,017] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,016] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,015] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,014] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:29:09,021] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Adding newly assigned partitions: test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,020] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,019] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,023] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:29:09,030] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Setting offset for partition test2-0 to the committed offset FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-02-20 22:29:13,358] INFO WorkerSinkTask{id=test7-1} Committing offsets asynchronously using sequence number 1: {test2-0=OffsetAndMetadata{offset=11, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:29:56,487] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2021-02-20 22:29:56,490] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2021-02-20 22:29:56,504] INFO Stopped http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2021-02-20 22:29:56,506] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2021-02-20 22:29:56,511] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2021-02-20 22:29:56,511] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:102)
[2021-02-20 22:29:56,513] INFO Stopping task test7-0 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,514] INFO Stopping task test7-1 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,515] INFO Stopping task test7-2 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,517] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,517] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,518] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,519] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,519] INFO Stopping task test7-3 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,520] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Member connector-consumer-test7-2-68545755-a247-4a55-bd63-b333414ed3dd sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,521] INFO Stopping task test7-4 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,520] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Revoke previously assigned partitions test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-02-20 22:29:56,524] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,525] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,525] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,526] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Member connector-consumer-test7-4-bdd5d075-cd3a-4f09-a099-cb41ae271fb5 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,524] INFO Stopping task test7-5 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,522] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,529] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,529] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,529] INFO Stopping task test7-6 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,526] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,524] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Member connector-consumer-test7-1-8b787bb0-ddc0-4399-b9c7-1922601f0881 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,539] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,532] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,540] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,531] INFO Stopping task test7-7 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,531] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,530] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Member connector-consumer-test7-3-48815871-48d5-470e-abea-0bad13b815c6 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,529] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,543] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,544] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,543] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,543] INFO Stopping task test7-8 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,541] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,547] INFO Stopping task test7-9 (org.apache.kafka.connect.runtime.Worker:836)
[2021-02-20 22:29:56,547] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,546] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,546] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,545] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Member connector-consumer-test7-5-a9e658f0-e757-452a-8ef5-9abaab2f5922 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,545] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Member connector-consumer-test7-6-eced6917-746d-4e9f-85af-7a5ddb14b23d sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,551] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Member connector-consumer-test7-7-e1b6fa14-5b59-4a04-8ab3-b69b2b167c5c sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,549] INFO Stopping Rockset Kafka Connect Plugin, waiting for active tasks to complete (rockset.RocksetSinkTask:176)
[2021-02-20 22:29:56,554] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,549] INFO Stopped Rockset Kafka Connect Plugin (rockset.RocksetSinkTask:178)
[2021-02-20 22:29:56,555] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,549] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,557] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,556] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Member connector-consumer-test7-8-cad8d27a-5de8-4377-99cd-1ee95c1b95b4 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,557] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Member connector-consumer-test7-9-6ef30fe1-75b8-44b2-8644-4a87ebc591a1 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2021-02-20 22:29:56,554] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,561] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,560] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,558] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,558] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,567] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,564] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,579] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,573] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,568] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,581] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,580] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,582] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,583] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,584] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,584] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,623] INFO App info kafka.consumer for connector-consumer-test7-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,633] INFO App info kafka.consumer for connector-consumer-test7-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,641] INFO App info kafka.consumer for connector-consumer-test7-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,650] INFO App info kafka.consumer for connector-consumer-test7-9 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,661] INFO App info kafka.consumer for connector-consumer-test7-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,668] INFO App info kafka.consumer for connector-consumer-test7-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,671] INFO App info kafka.consumer for connector-consumer-test7-8 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,672] INFO App info kafka.consumer for connector-consumer-test7-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,674] INFO App info kafka.consumer for connector-consumer-test7-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,769] INFO Stopping connector test7 (org.apache.kafka.connect.runtime.Worker:387)
[2021-02-20 22:29:56,771] INFO Scheduled shutdown for WorkerConnector{id=test7} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2021-02-20 22:29:56,772] INFO Completed shutdown for WorkerConnector{id=test7} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2021-02-20 22:29:56,776] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:209)
[2021-02-20 22:29:56,779] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2021-02-20 22:29:56,782] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:29:56,783] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:29:56,784] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:29:56,785] INFO App info kafka.connect for 127.0.1.1:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:29:56,786] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:230)
[2021-02-20 22:29:56,790] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:120)
[2021-02-20 22:29:56,790] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2021-02-20 22:30:35,361] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:69)
[2021-02-20 22:30:35,397] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../logs, -Dlog4j.configuration=file:./kafka_2.13-2.7.0/bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_281, 25.281-b09
	jvm.classpath = /home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/activation-1.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/argparse4j-0.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/audience-annotations-0.5.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-cli-1.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/commons-lang3-3.8.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-api-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-basic-auth-extension-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-file-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-json-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-mirror-client-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-runtime-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/connect-transforms-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-api-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-locator-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/hk2-utils-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-core-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-databind-2.10.5.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-paranamer-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jackson-module-scala_2.13-2.10.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.inject-2.6.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.25.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javassist-3.26.0-GA.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.servlet-api-3.1.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jaxb-api-2.3.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-client-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-common-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-container-servlet-core-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-hk2-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-media-jaxb-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jersey-server-2.31.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-client-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-continuation-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-http-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-io-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-security-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-server-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlet-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-servlets-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jetty-util-9.4.33.v20201020.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/jopt-simple-5.0.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka_2.13-2.7.0-sources.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-clients-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-log4j-appender-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-raft-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-examples-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-scala_2.13-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-streams-test-utils-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/kafka-tools-2.7.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/log4j-1.2.17.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/lz4-java-1.7.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/maven-artifact-3.6.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/metrics-core-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-buffer-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-codec-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-handler-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-resolver-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-epoll-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/netty-transport-native-unix-common-4.1.51.Final.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/paranamer-2.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/plexus-utils-3.2.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/reflections-0.9.12.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/rocksdbjni-5.18.4.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-collection-compat_2.13-2.2.0.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-java8-compat_2.13-0.9.1.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-library-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-logging_2.13-3.9.2.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/scala-reflect-2.13.3.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-api-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/snappy-java-1.1.7.7.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zookeeper-jute-3.5.8.jar:/home/farrah/Downloads/rocksters/kafka_2.13-2.7.0/bin/../libs/zstd-jni-1.4.5-6.jar
	os.spec = Linux, amd64, 4.4.0-103-generic
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-02-20 22:30:35,403] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:78)
[2021-02-20 22:30:35,515] INFO Loading plugin from: /home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-02-20 22:30:42,013] INFO Registered loader: PluginClassLoader{pluginLocation=file:/home/farrah/Downloads/rocksters/kafka-connect-rockset-1.2.0-jar-with-dependencies.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:30:42,016] INFO Added plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:42,017] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:42,018] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:42,019] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:42,020] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:42,020] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,226] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-02-20 22:30:47,228] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,228] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,230] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,231] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,237] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,239] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,239] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,240] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,241] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,242] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,243] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,244] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,245] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,246] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,247] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,248] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,249] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,250] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,250] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,251] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,253] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,253] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,254] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,255] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,256] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,257] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,257] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,258] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,258] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,259] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,260] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,261] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,261] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,262] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,263] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,264] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,265] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,266] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,266] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,267] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,268] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,269] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,270] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,271] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,272] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,273] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,273] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-02-20 22:30:47,278] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,280] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,281] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,283] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,284] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,285] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,286] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,287] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,288] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,289] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,290] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,292] INFO Added aliases 'RocksetSinkConnector' and 'RocksetSink' to plugin 'rockset.RocksetSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,293] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,294] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,295] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,296] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,297] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,298] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,299] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,300] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,300] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,301] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,302] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,303] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,304] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,304] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,305] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,306] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,307] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,307] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,309] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,312] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,315] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,318] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,321] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,322] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,323] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,324] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,324] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-02-20 22:30:47,325] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,326] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,327] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2021-02-20 22:30:47,457] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = [kafka-connect-rockset-1.2.0-jar-with-dependencies.jar]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:361)
[2021-02-20 22:30:47,461] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:30:47,480] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:30:47,940] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:47,941] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:47,941] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:47,942] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:47,942] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:47,943] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:47,943] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:47,946] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:47,946] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:47,947] INFO Kafka startTimeMs: 1613878247944 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:49,709] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:30:49,721] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:30:49,766] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:30:49,767] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:30:49,768] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:30:49,838] INFO Logging initialized @16281ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2021-02-20 22:30:50,114] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-02-20 22:30:50,115] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-02-20 22:30:50,161] INFO jetty-9.4.33.v20201020; built: 2020-10-20T23:39:24.803Z; git: 1be68755656cef678b79a2ef1c2ebbca99e25420; jvm 1.8.0_281-b09 (org.eclipse.jetty.server.Server:375)
[2021-02-20 22:30:50,313] INFO Started http_8083@1bdaa23d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-02-20 22:30:50,315] INFO Started @16758ms (org.eclipse.jetty.server.Server:415)
[2021-02-20 22:30:50,435] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:30:50,436] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-02-20 22:30:50,437] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:30:50,438] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-02-20 22:30:50,440] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-02-20 22:30:50,442] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-02-20 22:30:50,484] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-02-20 22:30:50,487] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2021-02-20 22:30:50,529] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:50,529] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:50,530] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:50,530] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:50,531] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:50,531] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:50,532] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2021-02-20 22:30:50,532] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:50,533] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:50,533] INFO Kafka startTimeMs: 1613878250532 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:50,581] INFO Kafka cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-02-20 22:30:50,584] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-02-20 22:30:50,613] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2021-02-20 22:30:50,614] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2021-02-20 22:30:50,615] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2021-02-20 22:30:50,633] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:50,634] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:50,635] INFO Kafka startTimeMs: 1613878250633 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:51,258] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:30:51,264] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2021-02-20 22:30:51,304] INFO Kafka Connect standalone worker initialization took 15934ms (org.apache.kafka.connect.cli.ConnectStandalone:100)
[2021-02-20 22:30:51,305] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-02-20 22:30:51,310] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-02-20 22:30:51,310] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2021-02-20 22:30:51,313] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-02-20 22:30:51,326] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2021-02-20 22:30:51,326] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-02-20 22:30:51,327] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-02-20 22:30:51,521] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-02-20 22:30:51,884] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-02-20 22:30:51,884] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-02-20 22:30:51,890] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2021-02-20 22:30:54,398] INFO Started o.e.j.s.ServletContextHandler@2b0b4d53{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2021-02-20 22:30:54,399] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-02-20 22:30:54,399] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-02-20 22:30:54,475] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2021-02-20 22:30:54,525] INFO Creating connector test7 of type rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2021-02-20 22:30:54,529] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:54,531] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:54,555] INFO Instantiated connector test7 with version 1.0 of type class rockset.RocksetSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2021-02-20 22:30:54,557] INFO Finished creating connector test7 (org.apache.kafka.connect.runtime.Worker:310)
[2021-02-20 22:30:54,562] INFO Starting RocksetSinkConnector (rockset.RocksetSinkConnector:22)
[2021-02-20 22:30:54,567] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:54,568] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:54,575] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:54,577] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:54,587] INFO Creating task test7-0 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:54,597] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:54,599] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:54,604] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:54,606] INFO Instantiated task test7-0 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:54,612] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:54,613] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:54,613] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-0 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:54,614] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-0 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:54,616] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:54,643] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:54,645] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:54,647] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:54,698] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:54,896] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:54,897] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:54,898] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:54,898] INFO Kafka startTimeMs: 1613878254897 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:54,941] INFO Creating task test7-1 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:54,945] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:54,947] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:54,947] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:54,948] INFO Instantiated task test7-1 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:54,949] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:54,949] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:54,950] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-1 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:54,950] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-1 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:54,951] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-1 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:54,953] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:54,957] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:54,957] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:54,959] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:54,959] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:54,964] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:54,967] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,007] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,008] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,009] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,009] INFO Kafka startTimeMs: 1613878255008 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,023] INFO Creating task test7-2 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,025] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,031] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,032] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,039] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,041] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,042] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,043] INFO Instantiated task test7-2 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,044] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,045] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,046] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-2 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,047] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-2 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,047] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-2 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,060] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,061] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,063] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,065] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,096] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,097] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,098] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,098] INFO Kafka startTimeMs: 1613878255097 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,111] INFO Creating task test7-3 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,117] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,121] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,124] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,125] INFO Instantiated task test7-3 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,126] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,127] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,128] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,129] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-3 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,135] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,137] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,138] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-3 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,139] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-3 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,146] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,149] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,157] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,163] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,196] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,197] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,198] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,199] INFO Kafka startTimeMs: 1613878255197 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,217] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,217] INFO Creating task test7-4 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,222] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,223] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,235] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,241] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,247] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,248] INFO Instantiated task test7-4 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,249] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,251] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,253] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-4 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,254] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-4 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,255] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-4 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,259] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,261] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,263] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,266] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,292] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,294] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,294] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,295] INFO Kafka startTimeMs: 1613878255293 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,306] INFO Creating task test7-5 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,309] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,312] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,313] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,314] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,316] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,316] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,321] INFO Instantiated task test7-5 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,325] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,326] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,326] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-5 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,327] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-5 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,327] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-5 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,334] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,336] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,339] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,341] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,366] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,367] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,368] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,368] INFO Kafka startTimeMs: 1613878255367 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,383] INFO Creating task test7-6 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,386] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,387] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,390] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,392] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,393] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,395] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,396] INFO Instantiated task test7-6 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,397] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,398] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,399] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-6 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,399] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-6 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,400] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-6 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,405] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,410] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,412] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,415] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,438] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,440] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,440] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,441] INFO Kafka startTimeMs: 1613878255439 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,452] INFO Creating task test7-7 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,456] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,463] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,464] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,465] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,472] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,473] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,474] INFO Instantiated task test7-7 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,475] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,478] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,479] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-7 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,479] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-7 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,480] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-7 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,485] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,487] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,489] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,492] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,528] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,529] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,530] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,531] INFO Kafka startTimeMs: 1613878255529 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,546] INFO Creating task test7-8 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,551] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,554] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,556] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,557] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,559] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,562] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,564] INFO Instantiated task test7-8 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,565] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,566] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,567] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-8 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,568] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-8 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,568] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-8 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,575] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,579] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,583] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,590] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,617] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,618] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,618] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,619] INFO Kafka startTimeMs: 1613878255618 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,631] INFO Creating task test7-9 (org.apache.kafka.connect.runtime.Worker:509)
[2021-02-20 22:30:55,634] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,639] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2021-02-20 22:30:55,639] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,640] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:55,640] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,643] INFO TaskConfig values: 
	task.class = class rockset.RocksetSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2021-02-20 22:30:55,644] INFO Instantiated task test7-9 with version 0.0.0.0 of type rockset.RocksetSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2021-02-20 22:30:55,645] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,646] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = value
 (org.apache.kafka.connect.storage.StringConverterConfig:361)
[2021-02-20 22:30:55,646] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task test7-9 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2021-02-20 22:30:55,647] INFO Set up the value converter class org.apache.kafka.connect.storage.StringConverter for task test7-9 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2021-02-20 22:30:55,648] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task test7-9 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2021-02-20 22:30:55,652] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2021-02-20 22:30:55,660] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2021-02-20 22:30:55,663] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = rockset.RocksetSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = test7
	predicates = []
	tasks.max = 10
	topics = [test2]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.storage.StringConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2021-02-20 22:30:55,667] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-test7-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-test7
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2021-02-20 22:30:55,692] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2021-02-20 22:30:55,693] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-02-20 22:30:55,693] INFO Kafka commitId: 448719dc99a19793 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-02-20 22:30:55,694] INFO Kafka startTimeMs: 1613878255692 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-02-20 22:30:55,707] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Subscribed to topic(s): test2 (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2021-02-20 22:30:55,712] INFO RocksetConnectorConfig values: 
	format = JSON
	rockset.apikey = null
	rockset.apiserver.url = https://api.rs2.usw2.rockset.com
	rockset.collection = null
	rockset.integration.key = kafka://tkr84GnsX88MjNhN2RAbcZuh3Vt5xp5axUdHtjNVkBFY1oahaMeaWHCzTst2D2C3@api.rs2.usw2.rockset.com
	rockset.task.threads = 5
	rockset.workspace = commons
 (rockset.RocksetConnectorConfig:361)
[2021-02-20 22:30:55,713] INFO Created connector test7 (org.apache.kafka.connect.cli.ConnectStandalone:112)
[2021-02-20 22:30:55,714] INFO Building Rockset connector config. Apiserver: https://api.rs2.usw2.rockset.comNumber of Threads: 5, Format: JSON (rockset.RocksetConnectorConfig:30)
[2021-02-20 22:30:57,084] INFO WorkerSinkTask{id=test7-7} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,086] INFO WorkerSinkTask{id=test7-4} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,086] INFO WorkerSinkTask{id=test7-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,086] INFO WorkerSinkTask{id=test7-9} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,086] INFO WorkerSinkTask{id=test7-3} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,085] INFO WorkerSinkTask{id=test7-1} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,085] INFO WorkerSinkTask{id=test7-2} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,084] INFO WorkerSinkTask{id=test7-8} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,085] INFO WorkerSinkTask{id=test7-6} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,084] INFO WorkerSinkTask{id=test7-5} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2021-02-20 22:30:57,210] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,212] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,210] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,213] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,213] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,212] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,218] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,217] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,221] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,217] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,216] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,214] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,214] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,226] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,213] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Cluster ID: HlfrsxueSwOvmVoHJv1wKw (org.apache.kafka.clients.Metadata:279)
[2021-02-20 22:30:57,227] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,225] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,220] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,219] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,236] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,235] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,233] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2021-02-20 22:30:57,239] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,239] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,238] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,238] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,236] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,252] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,255] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,259] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,352] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,352] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,354] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,358] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,370] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,371] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,375] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,379] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,381] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,380] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,388] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test7-5-3b574e3f-dc40-40a5-a8cb-4de6e2683d4d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,390] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-test7-2-55233ae7-a69a-4fb5-8e2c-3e98647a836c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,405] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=5, memberId='connector-consumer-test7-5-3b574e3f-dc40-40a5-a8cb-4de6e2683d4d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:781)
[2021-02-20 22:30:57,409] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:472)
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
[2021-02-20 22:30:57,411] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Finished assignment for group at generation 5: {connector-consumer-test7-2-55233ae7-a69a-4fb5-8e2c-3e98647a836c=Assignment(partitions=[test2-0]), connector-consumer-test7-5-3b574e3f-dc40-40a5-a8cb-4de6e2683d4d=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:30:57,416] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,440] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=5, memberId='connector-consumer-test7-2-55233ae7-a69a-4fb5-8e2c-3e98647a836c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:781)
[2021-02-20 22:30:57,441] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:472)
org.apache.kafka.common.errors.RebalanceInProgressException: The group is rebalancing, so a rejoin is needed.
[2021-02-20 22:30:57,441] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-02-20 22:30:57,450] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-9-f0a27880-514a-4229-b3a2-f0c2214b5c35', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,452] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-5-3b574e3f-dc40-40a5-a8cb-4de6e2683d4d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,454] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-3-d2489f17-58e2-4259-b108-ea14d7d965d6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,451] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-8-5e2ab3d7-10b5-476d-8bc4-ef2af9c5ec00', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,451] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-2-55233ae7-a69a-4fb5-8e2c-3e98647a836c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,451] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-1-8c080b6a-e63f-4706-936a-a4af638b7c86', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,476] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Finished assignment for group at generation 6: {connector-consumer-test7-9-f0a27880-514a-4229-b3a2-f0c2214b5c35=Assignment(partitions=[]), connector-consumer-test7-4-96edc6c5-8ae0-4a41-8247-54f81e127356=Assignment(partitions=[]), connector-consumer-test7-3-d2489f17-58e2-4259-b108-ea14d7d965d6=Assignment(partitions=[]), connector-consumer-test7-2-55233ae7-a69a-4fb5-8e2c-3e98647a836c=Assignment(partitions=[]), connector-consumer-test7-1-8c080b6a-e63f-4706-936a-a4af638b7c86=Assignment(partitions=[]), connector-consumer-test7-6-1d395643-b9e8-4082-8665-b102eab053e5=Assignment(partitions=[]), connector-consumer-test7-0-5eed4283-0251-4899-a567-9237f1a2507f=Assignment(partitions=[test2-0]), connector-consumer-test7-8-5e2ab3d7-10b5-476d-8bc4-ef2af9c5ec00=Assignment(partitions=[]), connector-consumer-test7-5-3b574e3f-dc40-40a5-a8cb-4de6e2683d4d=Assignment(partitions=[]), connector-consumer-test7-7-8ff5641e-cf48-4085-a167-551d7255d5b9=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-02-20 22:30:57,455] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-7-8ff5641e-cf48-4085-a167-551d7255d5b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,453] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-0-5eed4283-0251-4899-a567-9237f1a2507f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,453] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-6-1d395643-b9e8-4082-8665-b102eab053e5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,453] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Successfully joined group with generation Generation{generationId=6, memberId='connector-consumer-test7-4-96edc6c5-8ae0-4a41-8247-54f81e127356', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-02-20 22:30:57,491] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-5-3b574e3f-dc40-40a5-a8cb-4de6e2683d4d', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,491] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-8-5e2ab3d7-10b5-476d-8bc4-ef2af9c5ec00', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,491] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-1-8c080b6a-e63f-4706-936a-a4af638b7c86', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,491] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-3-d2489f17-58e2-4259-b108-ea14d7d965d6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,491] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-9-f0a27880-514a-4229-b3a2-f0c2214b5c35', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,502] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,504] INFO [Consumer clientId=connector-consumer-test7-3, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,501] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-6-1d395643-b9e8-4082-8665-b102eab053e5', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,498] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,501] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,509] INFO [Consumer clientId=connector-consumer-test7-1, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,494] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-2-55233ae7-a69a-4fb5-8e2c-3e98647a836c', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,516] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-4-96edc6c5-8ae0-4a41-8247-54f81e127356', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,513] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-0-5eed4283-0251-4899-a567-9237f1a2507f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,520] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[test2-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,521] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,522] INFO [Consumer clientId=connector-consumer-test7-4, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,513] INFO [Consumer clientId=connector-consumer-test7-5, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,510] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Successfully synced group in generation Generation{generationId=6, memberId='connector-consumer-test7-7-8ff5641e-cf48-4085-a167-551d7255d5b9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2021-02-20 22:30:57,526] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,527] INFO [Consumer clientId=connector-consumer-test7-7, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,509] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,507] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,503] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,611] INFO [Consumer clientId=connector-consumer-test7-8, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,610] INFO [Consumer clientId=connector-consumer-test7-6, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,603] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Adding newly assigned partitions: test2-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,517] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-02-20 22:30:57,615] INFO [Consumer clientId=connector-consumer-test7-2, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,611] INFO [Consumer clientId=connector-consumer-test7-9, groupId=connect-test7] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-02-20 22:30:57,652] INFO [Consumer clientId=connector-consumer-test7-0, groupId=connect-test7] Setting offset for partition test2-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-02-20 22:31:44,941] INFO WorkerSinkTask{id=test7-0} Committing offsets asynchronously using sequence number 5: {test2-0=OffsetAndMetadata{offset=12, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:33:04,964] INFO WorkerSinkTask{id=test7-0} Committing offsets asynchronously using sequence number 13: {test2-0=OffsetAndMetadata{offset=13, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:34:24,974] INFO WorkerSinkTask{id=test7-0} Committing offsets asynchronously using sequence number 21: {test2-0=OffsetAndMetadata{offset=14, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:39:15,007] INFO WorkerSinkTask{id=test7-0} Committing offsets asynchronously using sequence number 50: {test2-0=OffsetAndMetadata{offset=15, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
[2021-02-20 22:41:25,020] INFO WorkerSinkTask{id=test7-0} Committing offsets asynchronously using sequence number 63: {test2-0=OffsetAndMetadata{offset=16, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:352)
